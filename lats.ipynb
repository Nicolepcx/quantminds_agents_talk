{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Open Notebook directly in Google Colab:\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Nicolepcx/quantminds_agents_talk/blob/main/lats.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "_rOgnx3C4LRo"
      },
      "id": "_rOgnx3C4LRo"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E0UNkf7A4KHE"
      },
      "id": "E0UNkf7A4KHE"
    },
    {
      "cell_type": "markdown",
      "id": "9e0c1743-8775-4de2-a599-78b050551489",
      "metadata": {
        "id": "9e0c1743-8775-4de2-a599-78b050551489"
      },
      "source": [
        "# Language Agent Tree Search\n",
        "\n",
        "[Language Agent Tree Search](https://arxiv.org/abs/2310.04406) (LATS), by Zhou, et. al, is a general LLM agent search algorithm that combines reflection/evaluation and search (specifically monte-carlo trees search) to get achieve better overall task performance compared to similar techniques like ReACT, Reflexion, or Tree of Thoughts.\n",
        "\n",
        "![Graph](https://drive.google.com/uc?export=view&id=1u-zq_WQcuY6FlPB7-VJDCywix45P2W2J)\n",
        "\n",
        "It has four main steps:\n",
        "\n",
        "1. Select: pick the best next actions based on the aggregate rewards from step (2). Either respond (if a solution is found or the max search depth is reached) or continue searching.\n",
        "2. Expand and simulate: select the \"best\" 5 potential actions to take and execute them in parallel.\n",
        "3. Reflect + Evaluate: observe the outcomes of these actions and score the decisions based on reflection (and possibly external feedback)\n",
        "4. Backpropagate: update the scores of the root trajectories based on the outcomes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db28668b-5491-4c93-a961-bd339f09202c",
      "metadata": {
        "id": "db28668b-5491-4c93-a961-bd339f09202c"
      },
      "source": [
        "# Dependencies\n",
        "\n",
        "Install `langgraph` (for the framework), `langchain_openai` (for the LLM), and `langchain` + `tavily-python` (for the search engine).\n",
        "\n",
        "We will use tavily search as a tool. You can get an API key [here](https://app.tavily.com/sign-in) or replace with a different tool of your choosing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "dcc9159b-cc8c-426d-9670-3e8ada06723f",
      "metadata": {
        "id": "dcc9159b-cc8c-426d-9670-3e8ada06723f"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langgraph langchain_openai langchain_core -qqq\n",
        "!pip install tavily-python langchain_community python-dotenv -qqq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "mO17b-FHxF2o"
      },
      "id": "mO17b-FHxF2o"
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "JP8g7e6ax8Vg"
      },
      "id": "JP8g7e6ax8Vg",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "a917bb70-f84c-48e6-8d32-d14f9df2ca2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a6bde19-b3ee-40bc-b649-05319badffc9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import json\n",
        "from collections import deque, defaultdict\n",
        "from typing import Optional, List\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from langchain_core.output_parsers.openai_tools import (\n",
        "    JsonOutputToolsParser,\n",
        "    PydanticToolsParser,\n",
        ")\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.runnables import chain as as_runnable\n",
        "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
        "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
        "from langchain_core.prompt_values import ChatPromptValue\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "\n"
      ],
      "id": "a917bb70-f84c-48e6-8d32-d14f9df2ca2f"
    },
    {
      "cell_type": "code",
      "source": [
        "# API-Access\n",
        "\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "TAVILY_API_KEY = os.getenv('TAVILY_API_KEY')"
      ],
      "metadata": {
        "id": "qju62g3fxMDO"
      },
      "id": "qju62g3fxMDO",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Init LLM"
      ],
      "metadata": {
        "id": "hk3YoohfxY0H"
      },
      "id": "hk3YoohfxY0H"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "48738896-42ac-47eb-b482-0d4d4dd86c87",
      "metadata": {
        "id": "48738896-42ac-47eb-b482-0d4d4dd86c87"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f857eacb-af4a-47d1-b45f-da74941125c2",
      "metadata": {
        "id": "f857eacb-af4a-47d1-b45f-da74941125c2"
      },
      "source": [
        "# Graph State\n",
        "\n",
        "LATS is based on a (greedy) Monte-Carlo tree search. For each search steps, it picks the node with the highest \"upper confidence bound\", which is a metric that balances exploitation (highest average reward) and exploration (lowest visits). Starting from that node, it generates N (5 in this case) new candidate actions to take, and adds them to the tree. It stops searching either when it has generated a valid solution OR when it has reached the maximum number of rollouts (search tree depth).\n",
        "\n",
        "Your LangGraph state will be composed of two items:\n",
        "1. The root of the search tree\n",
        "2. The user input"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c611f1e-74b4-4157-997c-face8ad409a4",
      "metadata": {
        "id": "1c611f1e-74b4-4157-997c-face8ad409a4"
      },
      "source": [
        "## Reflection\n",
        "\n",
        "The reflection chain will score agent outputs based on the decision and the tool responses.\n",
        "We will call this within the other two nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ddfd1750-c265-4b29-b505-83b1c5e2d30e",
      "metadata": {
        "id": "ddfd1750-c265-4b29-b505-83b1c5e2d30e"
      },
      "outputs": [],
      "source": [
        "class Reflection(BaseModel):\n",
        "    reflections: str = Field(\n",
        "        description=\"The critique and reflections on the sufficiency, superfluency, and general quality of the response.\"\n",
        "    )\n",
        "    score: int = Field(\n",
        "        description=\"Score from 0-10 on the quality of the candidate response.\",\n",
        "        gte=0,\n",
        "        lte=10,\n",
        "    )\n",
        "    found_solution: bool = Field(\n",
        "        description=\"Whether the response has fully solved the question or task.\"\n",
        "    )\n",
        "\n",
        "    def as_message(self):\n",
        "        return HumanMessage(\n",
        "            content=f\"Reasoning: {self.reflections}\\nScore: {self.score}\"\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def normalized_score(self) -> float:\n",
        "        \"\"\"Return a normalized score between 0 and 1.\"\"\"\n",
        "        return self.score / 10.0\n",
        "\n",
        "    @classmethod\n",
        "    def from_response(cls, tool_choices, inputs):\n",
        "        \"\"\"Initialize Reflection instance directly from parsed tool response.\"\"\"\n",
        "        reflection = tool_choices[0]\n",
        "        if not isinstance(inputs[\"candidate\"][-1], AIMessage):\n",
        "            reflection.found_solution = False\n",
        "        return reflection\n",
        "\n",
        "# Configure Prompt Template for Reflection\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Reflect and grade the assistant response to the user question below.\",\n",
        "        ),\n",
        "        (\"user\", \"{input}\"),\n",
        "        MessagesPlaceholder(variable_name=\"candidate\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Reflection Chain Construction\n",
        "reflection_llm_chain = (\n",
        "    prompt\n",
        "    | llm.bind_tools(tools=[Reflection], tool_choice=\"Reflection\").with_config(\n",
        "        run_name=\"Reflection\"\n",
        "    )\n",
        "    | PydanticToolsParser(tools=[Reflection])\n",
        ")\n",
        "\n",
        "@as_runnable\n",
        "def reflection_chain(inputs) -> Reflection:\n",
        "    tool_choices = reflection_llm_chain.invoke(inputs)\n",
        "    return Reflection.from_response(tool_choices, inputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "54c6f319-3966-4f66-aa7b-50e249189111",
      "metadata": {
        "id": "54c6f319-3966-4f66-aa7b-50e249189111"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    __slots__ = (\n",
        "        \"messages\", \"parent\", \"children\", \"value\", \"visits\", \"reflection\", \"depth\",\n",
        "        \"_is_solved\"\n",
        "    )\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        reflection: 'Reflection',\n",
        "        parent: Optional[\"Node\"] = None,\n",
        "    ):\n",
        "        self.messages = messages\n",
        "        self.parent = parent\n",
        "        self.children = []\n",
        "        self.value = 0\n",
        "        self.visits = 0\n",
        "        self.reflection = reflection\n",
        "        self.depth = parent.depth + 1 if parent is not None else 1\n",
        "        self._is_solved = reflection.found_solution if reflection else False\n",
        "        if self._is_solved:\n",
        "            self._mark_tree_as_solved()\n",
        "        self.backpropagate(reflection.normalized_score)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (\n",
        "            f\"<Node value={self.value}, visits={self.visits},\"\n",
        "            f\" solution={self.messages} reflection={self.reflection}/>\"\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def is_solved(self):\n",
        "        return self._is_solved\n",
        "\n",
        "    @property\n",
        "    def is_terminal(self):\n",
        "        return not self.children\n",
        "\n",
        "    @property\n",
        "    def best_child(self):\n",
        "        if not self.children:\n",
        "            return None\n",
        "        return max(self.children, key=lambda child: child.upper_confidence_bound())\n",
        "\n",
        "    @property\n",
        "    def best_child_score(self):\n",
        "        if not self.children:\n",
        "            return None\n",
        "        return max(self.children, key=lambda child: int(child.is_solved) * child.value)\n",
        "\n",
        "    @property\n",
        "    def height(self) -> int:\n",
        "        if self.children:\n",
        "            return 1 + max(child.height for child in self.children)\n",
        "        return 1\n",
        "\n",
        "    def upper_confidence_bound(self, exploration_weight=1.0):\n",
        "        if self.parent is None:\n",
        "            raise ValueError(\"Cannot obtain UCT from root node\")\n",
        "        if self.visits == 0:\n",
        "            return self.value\n",
        "        average_reward = self.value / self.visits\n",
        "        exploration_term = math.sqrt(math.log(self.parent.visits) / self.visits)\n",
        "        return average_reward + exploration_weight * exploration_term\n",
        "\n",
        "    def backpropagate(self, reward: float):\n",
        "        node = self\n",
        "        while node:\n",
        "            node.visits += 1\n",
        "            node.value = (node.value * (node.visits - 1) + reward) / node.visits\n",
        "            node = node.parent\n",
        "\n",
        "    def get_messages(self, include_reflections: bool = True):\n",
        "        if include_reflections:\n",
        "            return self.messages + [self.reflection.as_message()]\n",
        "        return self.messages\n",
        "\n",
        "    def get_trajectory(self, include_reflections: bool = True) -> List[BaseMessage]:\n",
        "        messages = []\n",
        "        node = self\n",
        "        while node:\n",
        "            messages.extend(\n",
        "                node.get_messages(include_reflections=include_reflections)[::-1]\n",
        "            )\n",
        "            node = node.parent\n",
        "        return messages[::-1]\n",
        "\n",
        "    def _get_all_children(self):\n",
        "        all_nodes = []\n",
        "        nodes = deque([self])\n",
        "        while nodes:\n",
        "            node = nodes.popleft()\n",
        "            all_nodes.extend(node.children)\n",
        "            nodes.extend(node.children)\n",
        "        return all_nodes\n",
        "\n",
        "    def get_best_solution(self):\n",
        "        all_nodes = [self] + self._get_all_children()\n",
        "        best_node = max(\n",
        "            all_nodes,\n",
        "            key=lambda node: int(node.is_terminal and node.is_solved) * node.value,\n",
        "        )\n",
        "        return best_node\n",
        "\n",
        "    def _mark_tree_as_solved(self):\n",
        "        parent = self.parent\n",
        "        while parent:\n",
        "            parent._is_solved = True\n",
        "            parent = parent.parent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdd3111f-b860-471f-8784-1d5e3783910d",
      "metadata": {
        "id": "cdd3111f-b860-471f-8784-1d5e3783910d"
      },
      "source": [
        "#### The graph state itself\n",
        "\n",
        "The main component is the tree, represented by the root node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "e10c94ba-9daa-4899-97ce-4f28428c2c38",
      "metadata": {
        "id": "e10c94ba-9daa-4899-97ce-4f28428c2c38"
      },
      "outputs": [],
      "source": [
        "class TreeState(TypedDict):\n",
        "    # The full tree\n",
        "    root: Node\n",
        "    # The original input\n",
        "    input: str"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e8ddf25-d040-4e1f-87bd-5837ff105845",
      "metadata": {
        "id": "2e8ddf25-d040-4e1f-87bd-5837ff105845"
      },
      "source": [
        "# Define Language Agent\n",
        "\n",
        "Our agent will have three primary LLM-powered processes:\n",
        "1. Reflect: score the action based on the tool response.\n",
        "2. Initial response: to create the root node and start the search.\n",
        "3. Expand: generate 5 candidate \"next steps\" from the best spot in the current tree\n",
        "\n",
        "For more \"Grounded\" tool applications (such as code synthesis), you could integrate code execution into the reflection/reward step. This type of external feedback is very useful (though adds complexity to an already complicated example notebook)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d460856-e26d-4430-910e-0aac58563612",
      "metadata": {
        "id": "5d460856-e26d-4430-910e-0aac58563612"
      },
      "source": [
        "#### Tools\n",
        "\n",
        "For our example, we will give the language agent a search engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "55c2aff3-f454-43da-8f45-1a3d46523cd5",
      "metadata": {
        "id": "55c2aff3-f454-43da-8f45-1a3d46523cd5"
      },
      "outputs": [],
      "source": [
        "search = TavilySearchAPIWrapper()\n",
        "tavily_tool = TavilySearchResults(api_wrapper=search, max_results=5)\n",
        "tools = [tavily_tool]\n",
        "tool_executor = ToolExecutor(tools=tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e47dfb2-4ab3-4a31-b117-f07786b357cb",
      "metadata": {
        "id": "4e47dfb2-4ab3-4a31-b117-f07786b357cb"
      },
      "source": [
        "## Initial Response\n",
        "\n",
        "We start with a single root node, generated by this first step. It responds to the user input either with a tool invocation or a response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "72fc5363-f0f3-4362-8499-14eb583bd75b",
      "metadata": {
        "id": "72fc5363-f0f3-4362-8499-14eb583bd75b"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are an AI assistant.\",\n",
        "        ),\n",
        "        (\"user\", \"{input}\"),\n",
        "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "initial_answer_chain = prompt_template | llm.bind_tools(tools=tools).with_config(\n",
        "    run_name=\"GenerateInitialCandidate\"\n",
        ")\n",
        "\n",
        "\n",
        "parser = JsonOutputToolsParser(return_id=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a7d34a6-cee0-4321-989a-963ca4b2caeb",
      "metadata": {
        "id": "7a7d34a6-cee0-4321-989a-963ca4b2caeb"
      },
      "source": [
        "### Starting Node\n",
        "\n",
        "We will package up the candidate generation and reflection in a single node of our graph. This is represented by the following function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "5b6b173c-78f5-4ae1-80b3-28c80e68f5c5",
      "metadata": {
        "id": "5b6b173c-78f5-4ae1-80b3-28c80e68f5c5"
      },
      "outputs": [],
      "source": [
        "# Define the node we will add to the graph\n",
        "def generate_initial_response(state: TreeState) -> dict:\n",
        "    \"\"\"Generate the initial candidate response.\"\"\"\n",
        "    res = initial_answer_chain.invoke({\"input\": state[\"input\"]})\n",
        "    parsed = parser.invoke(res)\n",
        "    tool_responses = tool_executor.batch(\n",
        "        [ToolInvocation(tool=r[\"type\"], tool_input=r[\"args\"]) for r in parsed]\n",
        "    )\n",
        "    output_messages = [res] + [\n",
        "        ToolMessage(content=json.dumps(resp), tool_call_id=tool_call[\"id\"])\n",
        "        for resp, tool_call in zip(tool_responses, parsed)\n",
        "    ]\n",
        "    reflection = reflection_chain.invoke(\n",
        "        {\"input\": state[\"input\"], \"candidate\": output_messages}\n",
        "    )\n",
        "    root = Node(output_messages, reflection=reflection)\n",
        "    return {\n",
        "        **state,\n",
        "        \"root\": root,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34452e88-e33a-474c-9623-075d1f434dda",
      "metadata": {
        "id": "34452e88-e33a-474c-9623-075d1f434dda"
      },
      "source": [
        "## Candidate Generation\n",
        "\n",
        "The following code prompts the same LLM to generate N additional candidates to check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "d32af859-53e8-46be-8182-7d522be31f54",
      "metadata": {
        "id": "d32af859-53e8-46be-8182-7d522be31f54"
      },
      "outputs": [],
      "source": [
        "def expand(state: TreeState, config: RunnableConfig) -> dict:\n",
        "    \"\"\"Starting from the \"best\" node in the tree, generate N candidates for the next step.\"\"\"\n",
        "    root = state[\"root\"]\n",
        "    best_candidate: Node = root.best_child if root.children else root\n",
        "    messages = best_candidate.get_trajectory()\n",
        "    # Generate N candidates from the single child candidate\n",
        "    new_candidates = expansion_chain.invoke(\n",
        "        {\"input\": state[\"input\"], \"messages\": messages}, config\n",
        "    )\n",
        "    parsed = parser.batch(new_candidates)\n",
        "    flattened = [\n",
        "        (i, tool_call)\n",
        "        for i, tool_calls in enumerate(parsed)\n",
        "        for tool_call in tool_calls\n",
        "    ]\n",
        "    tool_responses = tool_executor.batch(\n",
        "        [\n",
        "            ToolInvocation(tool=tool_call[\"type\"], tool_input=tool_call[\"args\"])\n",
        "            for _, tool_call in flattened\n",
        "        ]\n",
        "    )\n",
        "    collected_responses = defaultdict(list)\n",
        "    for (i, tool_call), resp in zip(flattened, tool_responses):\n",
        "        collected_responses[i].append(\n",
        "            ToolMessage(content=json.dumps(resp), tool_call_id=tool_call[\"id\"])\n",
        "        )\n",
        "    output_messages = []\n",
        "    for i, candidate in enumerate(new_candidates):\n",
        "        output_messages.append([candidate] + collected_responses[i])\n",
        "\n",
        "    # Reflect on each candidate\n",
        "    # For tasks with external validation, you can add it here\n",
        "    reflections = reflection_chain.batch(\n",
        "        [{\"input\": state[\"input\"], \"candidate\": msges} for msges in output_messages],\n",
        "        config,\n",
        "    )\n",
        "    # Grow tree\n",
        "    child_nodes = [\n",
        "        Node(cand, parent=best_candidate, reflection=reflection)\n",
        "        for cand, reflection in zip(output_messages, reflections)\n",
        "    ]\n",
        "    best_candidate.children.extend(child_nodes)\n",
        "    # We have already extended the tree directly, so we just return the state\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "8aec0f20-f978-4df0-8900-e3a1f0544f6d",
      "metadata": {
        "id": "8aec0f20-f978-4df0-8900-e3a1f0544f6d"
      },
      "outputs": [],
      "source": [
        "# Centralized Configuration for LATS with MCTS\n",
        "config = {\n",
        "    \"candidate_count\": 5,  # N - Number of candidate generations\n",
        "    \"max_depth\": 5,        # Max rollout depth for tree search\n",
        "}\n",
        "\n",
        "# Generate Candidates with Centralized Configuration\n",
        "def generate_candidates(messages: ChatPromptValue, config: RunnableConfig):\n",
        "    n = config.get(\"candidate_count\", 5)  # Retrieve from centralized config\n",
        "    bound_kwargs = llm.bind_tools(tools=tools).kwargs\n",
        "\n",
        "    # Generate candidates with preconfigured settings\n",
        "    chat_result = llm.generate(\n",
        "        [messages.to_messages()],\n",
        "        n=n,\n",
        "        callbacks=config[\"callbacks\"],\n",
        "        run_name=\"GenerateCandidates\",\n",
        "        **bound_kwargs,\n",
        "    )\n",
        "    return [gen.message for gen in chat_result.generations[0]]\n",
        "\n",
        "# Build the StateGraph for MCTS with Centralized Depth Control\n",
        "from typing import Literal\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "\n",
        "def should_loop(state: TreeState) -> Literal[\"expand\", \"__end__\"]:\n",
        "    \"\"\"Determine whether to continue the tree search based on centralized config.\"\"\"\n",
        "    root = state[\"root\"]\n",
        "    if root.is_solved:\n",
        "        return END\n",
        "    if root.height > config[\"max_depth\"]:  # Use centralized depth limit\n",
        "        return END\n",
        "    return \"expand\"\n",
        "\n",
        "# Initialize the StateGraph with Tree Nodes\n",
        "builder = StateGraph(TreeState)\n",
        "builder.add_node(\"start\", generate_initial_response)\n",
        "builder.add_node(\"expand\", expand)\n",
        "builder.add_edge(START, \"start\")\n",
        "\n",
        "# Set conditional edges based on should_loop function\n",
        "builder.add_conditional_edges(\"start\", should_loop)\n",
        "builder.add_conditional_edges(\"expand\", should_loop)\n",
        "\n",
        "# Compile the graph\n",
        "graph = builder.compile()\n",
        "\n",
        "# Usage of expansion chain with centralized configuration\n",
        "expansion_chain = prompt_template | generate_candidates\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88ecf775-29ed-4ebd-8297-d1aa3cda3f9b",
      "metadata": {
        "id": "88ecf775-29ed-4ebd-8297-d1aa3cda3f9b"
      },
      "source": [
        "#### Candidate generation node\n",
        "\n",
        "We will package the candidate generation and reflection steps in the following \"expand\" node.\n",
        "We do all the operations as a batch process to speed up execution."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84bad5da-645d-4c6a-83dd-8c852f21f622",
      "metadata": {
        "id": "84bad5da-645d-4c6a-83dd-8c852f21f622"
      },
      "source": [
        "## Plot Graph\n",
        "\n",
        "With those two nodes defined, we are ready to define the graph. After each agent step, we have the option of finishing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "d1674593",
      "metadata": {
        "id": "d1674593",
        "outputId": "b195e362-e98a-4bb3-9b76-9e14ecd90936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "Image(graph.get_graph().draw_mermaid_png())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1383d69c-1d90-43f5-987e-c7fc4c3a24f8",
      "metadata": {
        "id": "1383d69c-1d90-43f5-987e-c7fc4c3a24f8"
      },
      "source": [
        "#  Run LATS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_lats(graph, question: str):\n",
        "    \"\"\"\n",
        "    Executes the graph query with the provided question, streams the results,\n",
        "    and retrieves the best solution's trajectory.\n",
        "\n",
        "    Parameters:\n",
        "    - graph: The LangGraph StateGraph instance.\n",
        "    - question (str): The query or prompt to feed into the graph.\n",
        "\n",
        "    Returns:\n",
        "    - str: The content of the best trajectory's final message.\n",
        "    \"\"\"\n",
        "    last_step = None\n",
        "    for step in graph.stream({\"input\": question}):\n",
        "        last_step = step\n",
        "        step_name, step_state = next(iter(step.items()))\n",
        "        print(step_name)\n",
        "        print(\"rolled out:\", step_state[\"root\"].height)\n",
        "        print(\"---\")\n",
        "\n",
        "    if last_step and \"expand\" in last_step:\n",
        "        solution_node = last_step[\"expand\"][\"root\"].get_best_solution()\n",
        "        best_trajectory = solution_node.get_trajectory(include_reflections=False)\n",
        "        return best_trajectory[-1].content\n",
        "    else:\n",
        "        print(\"No solution found or graph did not expand as expected.\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "Gd7bXwmxyz7C"
      },
      "id": "Gd7bXwmxyz7C",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"\n",
        "            Generate a table with the top five AI companies with their annual\n",
        "            revenue and their flagship Foundation Model.\n",
        "            \"\"\"\n",
        "best_solution_content = run_lats(graph, question)\n",
        "if best_solution_content:\n",
        "    print(\"Best solution content:\", best_solution_content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJNJnBZLbwL-",
        "outputId": "9eb31b8e-b69c-4d09-eb11-788b73118260"
      },
      "id": "JJNJnBZLbwL-",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start\n",
            "rolled out: 1\n",
            "---\n",
            "expand\n",
            "rolled out: 2\n",
            "---\n",
            "expand\n",
            "rolled out: 3\n",
            "---\n",
            "Best solution content: You're correct in pointing out the inaccuracies and assumptions in the previous response. Here's a revised table, acknowledging the challenges in obtaining specific revenue figures for AI divisions within larger companies:\n",
            "\n",
            "| Company      | Estimated Revenue (General) | Flagship Foundation Model |\n",
            "|--------------|-----------------------------|---------------------------|\n",
            "| OpenAI       | N/A                         | GPT-4                     |\n",
            "| Google (DeepMind) | Part of Alphabet's $1.88 trillion market cap | Gemini                      |\n",
            "| Microsoft    | N/A                         | Azure AI                  |\n",
            "| IBM          | N/A                         | Granite                   |\n",
            "| Anthropic    | N/A                         | Claude                    |\n",
            "\n",
            "### Notes:\n",
            "- **Revenue Figures**: Specific annual revenue for AI divisions like OpenAI, Google's DeepMind, and others are often not publicly disclosed. The figures or market caps mentioned are for the parent companies.\n",
            "- **Market Cap**: For Alphabet, the market cap is provided instead of specific revenue for DeepMind, as detailed figures are typically consolidated within Alphabet's overall financials.\n",
            "- **Flagship Models**: These are the prominent AI models associated with each company, representing their cutting-edge AI developments.\n",
            "\n",
            "This table aims to reflect the available information more accurately while acknowledging the limitations in financial data disclosure for AI-specific operations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kvqCMnPacbQt"
      },
      "id": "kvqCMnPacbQt",
      "execution_count": 37,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}